# RL vs Hand Clap Game

这是一个使用强化学习实现寻找拍手游戏策略的项目。

## 拍手游戏

拍手游戏是一种有趣的小游戏，在 WH（拼音缩写）中学流行。它起源于 $2023$ 年左右。

### 拍手游戏的规则

它的规则依赖于 `MOVEMENT_TABLE` 变量。它的内容如下（你也可以在 `main.py` 中找到它）：

```python
MOVEMENT_TABLE = {
    "生": {"kill": {"零"}, "need": -1, "combo": 0},
    "防": {"kill": set(), "need": 0, "combo": 0},
    "飞": {"kill": set(), "need": 0, "combo": 0},
    "单": {"kill": {"生", "地"}, "need": 1, "combo": 0},
    "双": {"kill": {"生", "一", "地"}, "need": 2, "combo": 0},
    "弯": {"kill": {"单", "双", "镖", "地", "机"}, "need": 1, "combo": 0},
    "刺": {"kill": {"弯", "肥", "地"}, "need": 1, "combo": 0},
    "肥": {"kill": {"防", "飞", "弯"}, "need": 5, "combo": 0},
    "镖": {"kill": {"飞"}, "need": 3, "combo": 0},
    "零": {"kill": {"防"}, "need": 0, "combo": 0},
    "一": {"kill": set(), "need": -1, "combo": 1},
    "二": {"kill": set(), "need": -2, "combo": 2},
    "三": {"kill": set(), "need": -3, "combo": 3},
    "四": {"kill": set(), "need": -4, "combo": 4},
    "五": {"kill": set(), "need": -5, "combo": 5},
    "胡": {"kill": {"生", "厨"}, "need": 1, "combo": 0},
    "菜": {"kill": {"胡"}, "need": 1, "combo": 0},
    "厨": {"kill": {"菜"}, "need": 2, "combo": 0},
    "地": {"kill": {"生", "一", "二", "三", "四", "五"}, "need": 3, "combo": 0},
    "机": {"kill": {"生", "防", "飞", "单", "双", "刺", "肥", "镖", "一", "二", "三", "四", "五", "胡", "菜", "厨", "地"}, "need": 10, "combo": 0}
}
```

它的规则类似石头剪刀布的复杂版本，两名（或多名）玩家同时出一个手势，手势之间有克制关系。  
与石头剪刀布不同的是：一句游戏有多个回合。玩家可以在回合中积累资源（以下称作“生化”），利用资源就可以出攻击手势。  
我们可以利用 `MOVEMENT[gestures]["kill"]` 来查看 `gestures` 能克制的其它手势。  
接下来，为了方便，我们把手势分成几类：

- 生化类：生（生化）
- 技能类：一（一技能）、二（二技能）、三（三技能）、四（四技能）、五（五技能）[1]
- 防御类：防（防御）、飞（飞天）
- 攻击类：其它所有

假设某名玩家连续出了 $n$ 个生化（中间可以出任意次防御类手势），那么它可以使用 $n$ 技能，来使它的生化数量额外增加 $n$ 个。  
举例：某名玩家出 `生生单生生防防飞防生` 之后，可以出一次三技能，然后他的生化数量会变成 $2-1+3\times2=7$ 个。  
多人游戏中，只要有一个在**上一回合**存活的玩家（称为 A）的手势能击杀另一个仍然存活的玩家（称为 B），那么**无论有没有其它人在这一句击杀 A**，B 在这一局都会死。  
举例：在某回合中，剩余 $4$ 个玩家分别出 `防单弯刺`，那么只有第一个玩家存活。  
这就是拍手游戏的规则，~~管你听没听懂，看就完了！~~

## 如何使用

### 训练

点击 `main.py` 以开始训练。

`main.py` 支持命令行参数：

1. 如果没有命令行参数，会从头开始新的训练，并且询问超参数配置（需要用户自己输入）。
2. 如果有一个命令行参数，会从头开始新的训练，并且把参数作为超参数配置所在的文件。
3. 如果有两个命令行参数，第一个参数是 Q 表的路径，第二个参数是超参数配置的路径。
   1. 如果第一个参数是 `.`，会从头开始训练，不加载 Q 表。
   2. 如果第二个参数是 `.`，会询问超参数配置，从文件加载 Q 表。

### 决策树生成

### GUI

### 超参数格式

超参数使用 JSON 格式。示例如下：

```json
{
  "ALPHA_0": 0.2,
  "ALPHA_MIN": 0.02,
  "DECAY_EPISODES": 10,
  "EPSILON_DECAY": 5e-07,
  "EPSILON_END": 0.01,
  "EPSILON_START": 0.2,
  "GAMMA": 0.9,
  "ROUND_PER_TEST": 20000,
  "TOTAL_EPISODES": 15,
  "TOTAL_GAME_ROUND": 5000000}
```

它们的意义如下：

| 名称                 | 意义                         |
|--------------------|----------------------------|
| `ALPHA_0`          | 初始学习率                      |
| `ALPHA_MIN`        | 最小学习率                      |
| `DECAY_EPISODES`   | 学习率线性下降的步数                 |
| `EPSILON_DECAY`    | 探索率下降的速度（每轮）               |
| `EPSILON_END`      | 探索率下限                      |
| `EPSILON_START`    | 初始探索率                      |
| `GAMMA`            | $\gamma$ 值，用来权衡当前奖励和未来长期奖励 |
| `ROUND_PER_TEST`   | 每次测试的轮数                    |
| `TOTAL_GAME_ROUND` | 训练总轮数                      |

---

**注释**

1. 其实原版游戏里还有更多技能，但是比较少用，这里不列举出来。
